{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2915,"status":"ok","timestamp":1701573608154,"user":{"displayName":"L動u Gia Huy","userId":"01458389295153929324"},"user_tz":-420},"id":"rGH4zwKMQOYe","outputId":"9724a1ac-8c24-4a5f-f201-c145735cff01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":157,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import re\n","import csv\n","import nltk\n","import time\n","import random\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import tensorflow as tf\n","from numpy.linalg import norm\n","import matplotlib.pyplot as plt\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","random.seed(time.time())\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7hfxrkzYroK"},"outputs":[],"source":["def cosine_similarity(vector_a, vector_b):\n","    dot_product = np.dot(vector_a, vector_b)\n","    norm_a = norm(vector_a)\n","    norm_b = norm(vector_b)\n","\n","    similarity = dot_product / (norm_a * norm_b)\n","    return similarity\n","\n","\n","def _2file_2data(list_root, list_rand, data):\n","    row = []\n","    for file_a in list_file_a:\n","      with open(file_a, \"r\") as f:\n","          normalize_content_a = f.read()\n","\n","      normalize_content_a = normalize_content_a.replace(\"\\n\", \" \")\n","\n","      for file_b in list_file_b:\n","          with open(file_b, \"r\") as f:\n","              normalize_content_b = f.read()\n","\n","          normalize_content_b = normalize_content_b.replace(\"\\n\", \" \")\n","          row.append(normalize_content_a)\n","          row.append(normalize_content_b)\n","\n","          if list_root == list_rand:\n","              row.append(1)\n","          else:\n","              row.append(0)\n","          data.append(row)\n","          row = []\n","    return data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8T59i5qQXlc"},"outputs":[],"source":["root_folder = '/content/drive/MyDrive/process_data/'\n","\n","dict_rand_file = {}\n","data = []\n","\n","for i in range(1,26):\n","    list_file_a = []\n","    list_file_b = []\n","    folder_path = f'/content/drive/MyDrive/process_data/data/{i}'\n","    for file_name in os.listdir(folder_path):\n","        if \"NORMALIZE\" in file_name:\n","            file_path = os.path.join(folder_path, file_name)\n","            list_file_a.append(file_path)\n","\n","    random_value = random.choice([x for x in range(1, 26) if (x != i and str(i) not in dict_rand_file) or (x != i and str(i) in dict_rand_file and x != dict_rand_file[str(i)])])\n","\n","    folder_path = f'/content/drive/MyDrive/process_data/data/{random_value}'\n","    for file_name in os.listdir(folder_path):\n","        if \"NORMALIZE\" in file_name:\n","            file_path = os.path.join(folder_path, file_name)\n","            list_file_b.append(file_path)\n","\n","    dict_rand_file[str(random_value)] = str(i)\n","\n","    data = _2file_2data(list_file_a, list_file_b, data)\n","    data = _2file_2data(list_file_b, list_file_a, data)\n","    data = _2file_2data(list_file_a, list_file_a, data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Kyd1AHVXGT8"},"outputs":[],"source":["feature_names = ['funct1', 'funct2', 'label']\n","\n","csv_file_path = '/content/drive/MyDrive/process_data/dataset.csv'\n","\n","with open(csv_file_path, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(feature_names)\n","    writer.writerows(data)\n","\n","df = pd.read_csv(csv_file_path, index_col=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yk2Gj--92WxO"},"outputs":[],"source":["train, test = train_test_split(df, test_size=0.4, random_state=44)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZOHT19-zTnX"},"outputs":[],"source":["test = df.sample(frac=0.3, random_state=44)\n","train = df.sample(frac=1, random_state=44)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQmpqIFc5cJX"},"outputs":[],"source":["combined_texts = [f\"{text_feature1} {text_feature2}\" for text_feature1, text_feature2 in zip(train.funct1, train.funct2)]\n","\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(combined_texts)\n","vocab_size = len(tokenizer.word_index) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1701573613719,"user":{"displayName":"L動u Gia Huy","userId":"01458389295153929324"},"user_tz":-420},"id":"13AVARf96ayV","outputId":"5a224c99-8d2f-4056-8fb1-fc17bcda2744"},"outputs":[{"name":"stdout","output_type":"stream","text":["Word index length: 70\n","Some words: ['typeone', 'typesix', 'mov', 'typethree', 'call', 'typefive', 'lea', 'typeseven', 'xor', 'push', 'pop', 'add', 'sub', 'ret', 'test', 'je', 'jmp', 'typefour', 'jne', 'endbr64', 'typetwo', 'movaps', 'nop', 'cmp', 'cs', 'movups', 'movsx', 'movsxd', 'shr', 'and', 'xorps', 'movzx', 'paddd', 'xchg', 'pxor', 'setne', 'cmove', 'jbe', 'movd', 'imul', 'punpcklwd', 'punpcklbw', 'inc', 'cdqe', 'psrad', 'sar', 'dec', 'ja', 'cdq', 'cmovns', 'neg', 'sete', 'cmovne', 'movdqa', 'or', 'shl', 'notrack', 'jb', 'idiv', 'pshufd', 'rep', 'cqo', 'pcmpgtw', 'punpckhwd', 'psrldq', 'jae', 'data16', 'movdqu', 'pcmpgtb', 'punpckhbw']\n"]}],"source":["print(f\"Word index length: {len(tokenizer.word_index)}\")\n","print(f\"Some words: {list(tokenizer.word_index.keys())[0:200]}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701573613719,"user":{"displayName":"L動u Gia Huy","userId":"01458389295153929324"},"user_tz":-420},"id":"7siXGWkPGT4Y","outputId":"a28e173a-84a1-405a-f674-b1cd24d2633a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Maximum token length: 360\n"]}],"source":["max_length = -1\n","\n","for tweet in df[\"funct1\"]:\n","    length = len(tweet.split())\n","    if length > max_length:\n","        max_length = length\n","\n","print(f\"Maximum token length: {max_length}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ehjs0pDEgeQH"},"outputs":[],"source":["sequences_feature1 = tokenizer.texts_to_sequences(train.funct1)\n","sequences_feature2 = tokenizer.texts_to_sequences(train.funct2)\n","\n","padded_feature1 = pad_sequences(sequences_feature1, maxlen=max_length, padding='post')\n","padded_feature2 = pad_sequences(sequences_feature2, maxlen=max_length, padding='post')\n","\n","input_feature1 = padded_feature1\n","input_feature2 = padded_feature2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hK8pK9FVGnk3"},"outputs":[],"source":["y = df.label.values\n","\n","X_train_feature1, X_val_feature1, X_train_feature2, X_val_feature2, y_train, y_val = train_test_split(\n","    input_feature1, input_feature2, y, test_size=0.2, random_state=42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5qIG6gLi_4E"},"outputs":[],"source":["list_word = list(tokenizer.word_index.keys())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJNdg2rHkF6l"},"outputs":[],"source":["root_folder = '/content/drive/MyDrive/process_data/data/'\n","\n","content = \"\"\n","for i in range(1,26):\n","  folder_path = f'/content/drive/MyDrive/process_data/data/{str(i)}'\n","  for file_name in os.listdir(folder_path):\n","    if \"NORMALIZE\" in file_name:\n","        file_path = os.path.join(folder_path, file_name)\n","        with open(file_path, \"r\") as f:\n","            content += f.read() + \"\\n\\n\"\n","tokenized_code = [word_tokenize(line.lower()) for line in content.split('\\n') if line]\n","model = Word2Vec(sentences=tokenized_code, vector_size=300, window=5, min_count=1, workers=4, sg=1)\n","\n","embedding_dimension = 300\n","embeddings_matrix = np.zeros((vocab_size, embedding_dimension))\n","for word, index in tokenizer.word_index.items():\n","    embeddings_matrix[index] = model.wv[word]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yj0eUGzdd4tp"},"outputs":[],"source":["input_feature1 = tf.keras.layers.Input(shape=(max_length,), name='funct1')\n","embedding_layer_feature1 = tf.keras.layers.Embedding(\n","    vocab_size,\n","    embedding_dimension,\n","    input_length=max_length,\n","    weights=[embeddings_matrix],\n","    trainable=False\n",")(input_feature1)\n","\n","input_feature2 = tf.keras.layers.Input(shape=(max_length,), name='funct2')\n","embedding_layer_feature2 = tf.keras.layers.Embedding(\n","    vocab_size,\n","    embedding_dimension,\n","    input_length=max_length,\n","    weights=[embeddings_matrix],\n","    trainable=False\n",")(input_feature2)\n","\n","concatenated_features = tf.keras.layers.Concatenate()([embedding_layer_feature1, embedding_layer_feature2])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2124,"status":"ok","timestamp":1701573616294,"user":{"displayName":"L動u Gia Huy","userId":"01458389295153929324"},"user_tz":-420},"id":"DTLE-iVArQdP","outputId":"49df86dc-54ef-448b-d51a-d49c521d1d95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_8\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," funct1 (InputLayer)         [(None, 360)]                0         []                            \n","                                                                                                  \n"," funct2 (InputLayer)         [(None, 360)]                0         []                            \n","                                                                                                  \n"," embedding_18 (Embedding)    (None, 360, 100)             7100      ['funct1[0][0]']              \n","                                                                                                  \n"," embedding_19 (Embedding)    (None, 360, 100)             7100      ['funct2[0][0]']              \n","                                                                                                  \n"," concatenate_9 (Concatenate  (None, 360, 200)             0         ['embedding_18[0][0]',        \n"," )                                                                   'embedding_19[0][0]']        \n","                                                                                                  \n"," sequential_8 (Sequential)   (None, 1)                    1975745   ['concatenate_9[0][0]']       \n","                                                                                                  \n","==================================================================================================\n","Total params: 1989945 (7.59 MB)\n","Trainable params: 1975745 (7.54 MB)\n","Non-trainable params: 14200 (55.47 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["cnn_lstm_model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'),\n","    tf.keras.layers.MaxPooling1D(pool_size=2),\n","    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),\n","    tf.keras.layers.MaxPooling1D(pool_size=2),\n","    tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),\n","    tf.keras.layers.MaxPooling1D(pool_size=2),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256, return_sequences=True)),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128, return_sequences=True)),\n","    tf.keras.layers.GlobalMaxPooling1D(),\n","    tf.keras.layers.Dense(256, activation=\"relu\"),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(128, activation=\"relu\"),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(64, activation=\"relu\"),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(1, activation=\"tanh\"),\n","])\n","\n","main_model = tf.keras.models.Model(inputs=[input_feature1, input_feature2 ], outputs=cnn_lstm_model(concatenated_features))\n","\n","main_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n","    loss=\"binary_crossentropy\",\n","    metrics=[\"accuracy\"]\n",")\n","\n","main_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23212,"status":"ok","timestamp":1701573639501,"user":{"displayName":"L動u Gia Huy","userId":"01458389295153929324"},"user_tz":-420},"id":"wUZTSJ-pmi8k","outputId":"00c20443-4fb1-4e53-b276-efb549bff42c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","8/8 [==============================] - 13s 446ms/step - loss: 1.6356 - accuracy: 0.6656 - val_loss: 0.9747 - val_accuracy: 0.6708\n","Epoch 2/10\n","8/8 [==============================] - 1s 141ms/step - loss: 0.9227 - accuracy: 0.6656 - val_loss: 0.7922 - val_accuracy: 0.6708\n","Epoch 3/10\n","8/8 [==============================] - 1s 147ms/step - loss: 0.7746 - accuracy: 0.6656 - val_loss: 0.6975 - val_accuracy: 0.6708\n","Epoch 4/10\n","8/8 [==============================] - 1s 149ms/step - loss: 0.7064 - accuracy: 0.6615 - val_loss: 0.6658 - val_accuracy: 0.6625\n","Epoch 5/10\n","8/8 [==============================] - 1s 144ms/step - loss: 0.6800 - accuracy: 0.6404 - val_loss: 0.6669 - val_accuracy: 0.6302\n","Epoch 6/10\n","8/8 [==============================] - 1s 151ms/step - loss: 0.6817 - accuracy: 0.6201 - val_loss: 0.6662 - val_accuracy: 0.6240\n","Epoch 7/10\n","8/8 [==============================] - 1s 153ms/step - loss: 0.6803 - accuracy: 0.6263 - val_loss: 0.6603 - val_accuracy: 0.6531\n","Epoch 8/10\n","8/8 [==============================] - 1s 153ms/step - loss: 0.6769 - accuracy: 0.6385 - val_loss: 0.6563 - val_accuracy: 0.6625\n","Epoch 9/10\n","8/8 [==============================] - 1s 146ms/step - loss: 0.6710 - accuracy: 0.6484 - val_loss: 0.6541 - val_accuracy: 0.6677\n","Epoch 10/10\n","8/8 [==============================] - 1s 149ms/step - loss: 0.6703 - accuracy: 0.6497 - val_loss: 0.6521 - val_accuracy: 0.6677\n"]}],"source":["history = main_model.fit(\n","    [X_train_feature1, X_train_feature2],\n","    y_train,\n","    epochs=10,\n","    batch_size=512,\n","    validation_data=([X_val_feature1, X_val_feature2], y_val),\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPhv60FN38VpEikICpM/3Iq","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
